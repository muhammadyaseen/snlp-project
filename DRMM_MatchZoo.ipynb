{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DRMM-MatchZoo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1465f3ff6454636ba77c8a9b1cc176b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a6a2ad394b9c4df482da80050a8b97c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_715ce1dc328144829440a40cb881d6f2",
              "IPY_MODEL_c43781b93c0c47dbaf62a0c32d84d426"
            ]
          }
        },
        "a6a2ad394b9c4df482da80050a8b97c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "715ce1dc328144829440a40cb881d6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4ed7fefc4944438a9b8ceb86022b64a1",
            "_dom_classes": [],
            "description": "Epoch 1/20: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 160,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 160,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85cfefe7547f44ffaaa0392b6257b4c6"
          }
        },
        "c43781b93c0c47dbaf62a0c32d84d426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_991b5ced94f84c7baafeca29d7a20b2d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160/160 [09:05&lt;00:00,  3.41s/it, loss=2.392]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8280c8b0d1fe41af9c6c82d214f3c7e2"
          }
        },
        "4ed7fefc4944438a9b8ceb86022b64a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85cfefe7547f44ffaaa0392b6257b4c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "991b5ced94f84c7baafeca29d7a20b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8280c8b0d1fe41af9c6c82d214f3c7e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10dd0d21af8e49c8a4b99026125e514d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7528c29877af4083a2f149d8d145167a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0fa785eaeab94d52bec388f0990debb7",
              "IPY_MODEL_a7677e4d900b485aa22a14cd7f496364"
            ]
          }
        },
        "7528c29877af4083a2f149d8d145167a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0fa785eaeab94d52bec388f0990debb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_704bef1b45cb43c4945488ab0265fee7",
            "_dom_classes": [],
            "description": "Epoch 2/20:   8%",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 160,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d62b46c6d6c4ac4bd3c62575d692726"
          }
        },
        "a7677e4d900b485aa22a14cd7f496364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_73b3cfc9e58b46e698f21aa94c8c693b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13/160 [08:02&lt;1:30:59, 37.14s/it, loss=2.288]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb4aedc0b30b401eae23a9d7668e10b8"
          }
        },
        "704bef1b45cb43c4945488ab0265fee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d62b46c6d6c4ac4bd3c62575d692726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73b3cfc9e58b46e698f21aa94c8c693b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb4aedc0b30b401eae23a9d7668e10b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGPIpvAAyzou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "186787c4-825e-4689-bac2-7b0a6ae6a1da"
      },
      "source": [
        "!pip install matchzoo-py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting matchzoo-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/25/ee41c52865d6fe60c54eb362b00bed95d87198b88c301079631e30f597d0/matchzoo-py-1.1.1.tar.gz (109kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from matchzoo-py) (1.6.0+cu101)\n",
            "Collecting pytorch-transformers>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 13.1MB/s \n",
            "\u001b[?25hCollecting nltk>=3.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 15.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from matchzoo-py) (1.18.5)\n",
            "Collecting tqdm==4.38.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/08/8505f192efc72bfafec79655e1d8351d219e2b80b0dec4ae71f50934c17a/tqdm-4.38.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from matchzoo-py) (0.3.2)\n",
            "Collecting pandas==0.24.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/74/e50234bc82c553fecdbd566d8650801e3fe2d6d8c8d940638e3d8a7c5522/pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.6/dist-packages (from matchzoo-py) (2.4)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.6/dist-packages (from matchzoo-py) (2.10.0)\n",
            "Requirement already satisfied: hyperopt==0.1.2 in /usr/local/lib/python3.6/dist-packages (from matchzoo-py) (0.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->matchzoo-py) (0.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers>=1.1.0->matchzoo-py) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers>=1.1.0->matchzoo-py) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers>=1.1.0->matchzoo-py) (1.14.37)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk>=3.4.3->matchzoo-py) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk>=3.4.3->matchzoo-py) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2->matchzoo-py) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2->matchzoo-py) (2.8.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.3->matchzoo-py) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.9.0->matchzoo-py) (1.15.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt==0.1.2->matchzoo-py) (3.11.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt==0.1.2->matchzoo-py) (1.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers>=1.1.0->matchzoo-py) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers>=1.1.0->matchzoo-py) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers>=1.1.0->matchzoo-py) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers>=1.1.0->matchzoo-py) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers>=1.1.0->matchzoo-py) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers>=1.1.0->matchzoo-py) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers>=1.1.0->matchzoo-py) (1.17.37)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch-transformers>=1.1.0->matchzoo-py) (0.15.2)\n",
            "Building wheels for collected packages: matchzoo-py, nltk, sacremoses\n",
            "  Building wheel for matchzoo-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matchzoo-py: filename=matchzoo_py-1.1.1-cp36-none-any.whl size=163963 sha256=967e346f642d7217edbe4efd61436ec69fd17c506de42b7cd0a4a9d37abaaab6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/34/cf/47c8174d29709c9239e1c65dabfd8a83d1eb74d96fe154182a\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434676 sha256=8ca108209d5666cff32d5480c6ac1c220f66a6a4eccc3c0d4fe5e8612af4fde7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=4725975488bb9a7856e3f77d89925ca4b002b665058fcd9198e1508c4e9c78c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built matchzoo-py nltk sacremoses\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm, sacremoses, sentencepiece, pytorch-transformers, nltk, pandas, matchzoo-py\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: pandas 1.0.5\n",
            "    Uninstalling pandas-1.0.5:\n",
            "      Successfully uninstalled pandas-1.0.5\n",
            "Successfully installed matchzoo-py-1.1.1 nltk-3.5 pandas-0.24.2 pytorch-transformers-1.2.0 sacremoses-0.0.43 sentencepiece-0.1.91 tqdm-4.38.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFTwCE_kauJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79d6e5a5-42d4-4a2f-a9cc-7eb58c4e4da5"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = None\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chZ9rZJhW4HY",
        "colab_type": "text"
      },
      "source": [
        "# Baseline and Preprocessing\n",
        "\n",
        "This code prepares and lightly preprocesses given TREC XML corpus and runs a basic ranking method on the corpus for each of the given 100 queries.\n",
        "\n",
        "- We represent both q,d via TF-IDF vectors and use Cosine Similarity as ranking function. \n",
        "\n",
        "- We return top 1000 docs, and also calculate Precision@50, as required.\n",
        "\n",
        "- We prepare the returned relevant docs and queries in the right data format so that it can be consumed by MatchZoo-Py DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHu9Wc2klv-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "aa18b6ad-0de8-48b1-b16f-a11b95017cf3"
      },
      "source": [
        "# baseline.py\n",
        "\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import matchzoo as mz\n",
        "\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "data_root = \"./drive/My Drive/SNLP-Project-Data/\"\n",
        "ans_patterns = data_root + \"patterns.txt\"\n",
        "test_questions = data_root + \"test_questions.txt\"\n",
        "trec_corpus_xml = data_root + \"trec_documents.xml\"\n",
        "\n",
        "processed_root = data_root + \"processed/\"\n",
        "os.makedirs(processed_root, exist_ok=True)\n",
        "\n",
        "processed_corpus = processed_root + \"corpus.pkl\"\n",
        "processed_text_qs = processed_root + \"test_qs.pkl\"\n",
        "processed_tfids = processed_root + \"tfids.pkl\"\n",
        "processed_tfidf_repr = processed_root + \"tfrepr.pkl\"\n",
        "\n",
        "question_extraction_pattern = \"Number: (\\d+) *\\n\\n\\<desc\\> Description\\:\\n(\\w+.*)\\n\\n\\<\\/top>\"\n",
        "\n",
        "\n",
        "def get_test_questions(test_questions, ans_patterns, save=False):\n",
        "    try:\n",
        "        print(\"Loading from saved pickle\")\n",
        "        test_qs = pickle.load(open(processed_text_qs, \"rb\"))\n",
        "        return test_qs\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        # get question id and text\n",
        "        qs = {}\n",
        "        questions_doc = open(test_questions).read()\n",
        "        question_extraction_pattern = \"^\\<num\\> Number: (\\d+) *\\n\\n\\<desc\\> Description\\:\\n(\\w+.*)\\n\\n\\<\\/top>$\"\n",
        "        result = re.findall(question_extraction_pattern, questions_doc, re.MULTILINE)\n",
        "\n",
        "        for q in result:\n",
        "            processed_q = q[1].lower()\n",
        "            processed_q = processed_q.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "            qs[int(q[0])] = {'raw_question': q[1], 'question': processed_q, 'ans_patterns': []}\n",
        "\n",
        "        # get associated answer patterns\n",
        "        ans_doc = open(ans_patterns).readlines()\n",
        "\n",
        "        for ap in ans_doc:\n",
        "            # print(ap)\n",
        "            ap = ap.split(\" \")\n",
        "            id, pattern = ap[0], \" \".join(ap[1:]).strip()\n",
        "            qs[int(id)]['ans_patterns'].append(pattern)\n",
        "\n",
        "        if save:\n",
        "            print(\"saving processed questions, existing data will be overwritten\")\n",
        "            pickle.dump(\n",
        "                qs,\n",
        "                open(processed_text_qs, \"wb\")\n",
        "            )\n",
        "\n",
        "        return qs\n",
        "\n",
        "\n",
        "def process_trec_xml(trec_corpus_xml, save=False):\n",
        "    try:\n",
        "        print(\"Loading from saved pickle\")\n",
        "        corpus = pickle.load(open(processed_corpus, \"rb\"))\n",
        "        return corpus\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(\"Data doesn't exit or other error\", e)\n",
        "        print(\"Processing from scratch\")\n",
        "\n",
        "        corpus = {\n",
        "            # doc_id -> doc_text\n",
        "        }\n",
        "\n",
        "        with open(trec_corpus_xml, 'r') as dh:\n",
        "\n",
        "            soup = BeautifulSoup(dh, 'html.parser')\n",
        "            \n",
        "            # article_texts = soup.find_all('doc')\n",
        "            # Using 'text' instead of 'doc' we can remove the byline and headline \n",
        "            # information and also publication, page information\n",
        "            \n",
        "            article_texts = soup.find_all('text')\n",
        "            article_ids   = soup.find_all('docno')\n",
        "\n",
        "            assert len(article_texts) == len(article_ids)\n",
        "\n",
        "            print(\"Found %d articles...\" % len(article_texts))\n",
        "\n",
        "            #for a in article_texts:\n",
        "            for a_id, a_text in zip(article_ids, article_texts):    \n",
        "                \n",
        "                # for now we don't separate byline / headline etc\n",
        "                a_id = a_id.get_text().lower().strip()\n",
        "                \n",
        "                # remove common punct\n",
        "                # TODO: Remove Byline, Dates, and other useless meta stuff - DONE\n",
        "                text = a_text.get_text().lower()\n",
        "                text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "                text = text.replace('\\n', '')\n",
        "                text = text.replace('\\r', '')\n",
        "\n",
        "                corpus[a_id] = text\n",
        "\n",
        "        if save:\n",
        "            print(\"saving processed corpus, existing data will be overwritten\")\n",
        "            pickle.dump(\n",
        "                corpus,\n",
        "                open(processed_corpus, \"wb\")\n",
        "            )\n",
        "\n",
        "        return corpus\n",
        "\n",
        "\n",
        "# create representation of all docs in terms of their term freqs\n",
        "def compute_tfidf_doc_repr(corpus, term_idfs, save=False):\n",
        "    try:\n",
        "        print(\"Loading from saved pickle\")\n",
        "        corpus_tfidf_repr = pickle.load(open(processed_tfidf_repr, \"rb\"))\n",
        "\n",
        "        return corpus_tfidf_repr\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        corpus_tfidf_repr = {}\n",
        "\n",
        "        for doc_id in corpus:\n",
        "\n",
        "            tf_repr = Counter(corpus[doc_id].split(\" \"))\n",
        "            doc_max = max(tf_repr.values())\n",
        "\n",
        "            for k, v in tf_repr.items():\n",
        "                # normalize by max freq\n",
        "                tf_repr[k] = tf_repr[k] / doc_max\n",
        "                # weight tf by idf\n",
        "                tf_repr[k] = tf_repr[k] * term_idfs[k]\n",
        "\n",
        "            corpus_tfidf_repr[doc_id] = tf_repr\n",
        "\n",
        "        if save:\n",
        "            print(\"saving tfid repr, existing data will be overwritten\")\n",
        "            pickle.dump(\n",
        "                corpus_tfidf_repr,\n",
        "                open(processed_tfidf_repr, \"wb\")\n",
        "            )\n",
        "\n",
        "        return corpus_tfidf_repr\n",
        "\n",
        "\n",
        "# returns the idf weighted representation, given tf based repr as input\n",
        "def get_tfidfs_repr(v, term_idfs):\n",
        "    v = Counter(v.split(\" \"))\n",
        "    q_max = max(v.values())\n",
        "\n",
        "    for k, val in v.items():\n",
        "        # normalize by max freq\n",
        "        v[k] = v[k] / q_max\n",
        "        # weight tf by idf\n",
        "        try:\n",
        "            v[k] = v[k] * term_idfs[k]\n",
        "        except KeyError as ke:\n",
        "            # we might not have IDF score for some question terms.\n",
        "            # so we just use TF value, this is same as setting IDF = 1\n",
        "            pass\n",
        "\n",
        "    return v\n",
        "\n",
        "\n",
        "def cosine_sim(q, d):\n",
        "    # only terms common b/w q and d affect the dot product\n",
        "    # all other entries are either zero in query or in doc    \n",
        "    common_terms = set(q.keys()).intersection(set(d.keys()))\n",
        "\n",
        "    dot_prod = 0\n",
        "\n",
        "    for ct in common_terms:\n",
        "        dot_prod += q[ct] * d[ct]\n",
        "\n",
        "    mag_q = sum([v ** 2 for v in q.values()])\n",
        "    mag_d = sum([v ** 2 for v in d.values()])\n",
        "\n",
        "    denom = math.sqrt(mag_q) * math.sqrt(mag_d)\n",
        "\n",
        "    score = dot_prod / denom\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def compute_term_idfs(corpus, save=False):\n",
        "    try:\n",
        "        print(\"Loading from saved pickle\")\n",
        "        term_doc_freq = pickle.load(open(processed_tfids, \"rb\"))\n",
        "        return term_doc_freq\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        term_doc_freq = {}\n",
        "        N = len(corpus.keys())\n",
        "\n",
        "        # first we get the document freq of a term \n",
        "        # i.e. how many docs contain that term\n",
        "        # this is upper bounded by num of docs, of course\n",
        "        for doc in corpus:\n",
        "\n",
        "            # we are interested in just occurrence, and not actual freqs\n",
        "            # that's why we convert the doc to set of non-repeating terms\n",
        "            terms = set(corpus[doc].split(\" \"))\n",
        "\n",
        "            for term in terms:\n",
        "\n",
        "                if term in term_doc_freq.keys():\n",
        "                    term_doc_freq[term] += 1\n",
        "                else:\n",
        "                    term_doc_freq[term] = 1\n",
        "\n",
        "        # now that we have term's df, we inverse it and apply log normalization\n",
        "        for t in term_doc_freq.keys():\n",
        "            term_doc_freq[t] = math.log(N / term_doc_freq[t])\n",
        "\n",
        "        if save:\n",
        "            print(\"saving tfids, existing data will be overwritten\")\n",
        "            pickle.dump(\n",
        "                term_doc_freq,\n",
        "                open(processed_tfids, \"wb\")\n",
        "            )\n",
        "\n",
        "        return term_doc_freq\n",
        "\n",
        "# Returnd doc ids and scores sorted in DESCENDING order\n",
        "# i.e. the best doc will be at index 0 and so on\n",
        "def get_relevant_docs(q, tfidf_reprs, term_idfs, how_many=1):\n",
        "    assert how_many < len(tfidf_reprs)\n",
        "\n",
        "    doc_scores = {\n",
        "        # doc id -> doc score\n",
        "    }\n",
        "\n",
        "    q = get_tfidfs_repr(q['question'], term_idfs)\n",
        "\n",
        "    for d in tfidf_reprs:\n",
        "        doc_scores[d] = cosine_sim(q, tfidf_reprs[d])\n",
        "\n",
        "    sorted_scores = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:how_many]\n",
        "\n",
        "    # unpack the dict into separate lists\n",
        "    doc_ids, scores = zip(*sorted_scores)\n",
        "\n",
        "    return doc_ids, scores\n",
        "\n",
        "# Calculate precision over the returned result set and query\n",
        "# precision = #(rel and ret) / #(ret)\n",
        "def precision_at_r(returned_docs, q, corpus):\n",
        "    \n",
        "    # This 'R' should be fixed at 50 for report\n",
        "    # TODO: Confirm this\n",
        "    \n",
        "    R = len(returned_docs)\n",
        "    assert R == 50, \"R is not 50\"\n",
        "\n",
        "    relevant_count = 0\n",
        "\n",
        "    # print(R, q)\n",
        "    # check if doc is relevant wrt any of the answer patterns\n",
        "    for d in returned_docs:\n",
        "        rel = []\n",
        "        for ap in q['ans_patterns']:\n",
        "            rel.append(bool(re.search(ap.strip(), corpus[d], flags=re.IGNORECASE)))\n",
        "        \n",
        "        # count as relevant if at least one answer pattern matched somewhere in doc\n",
        "        relevant_count += int(any(rel))\n",
        "\n",
        "    # print(relevant_count)\n",
        "    return relevant_count / R\n",
        "\n",
        "\n",
        "# a utility function to find out how many docs are relevant \n",
        "# to a given query. Can be considered a histogram.\n",
        "# we expect to see a highly skewed histogram, with 1-5 docs relevant per query.    \n",
        "def relevant_per_query(corpus, queries):\n",
        "\n",
        "    # init rel counts with zero per query\n",
        "    from collections import defaultdict\n",
        "    \n",
        "    rel_counts = [0 for i in range(len(queries))]\n",
        "    rel_anspat = defaultdict(int)\n",
        "    # check if doc is relevant wrt any of the answer patterns\n",
        "    for doc in corpus:\n",
        "        for q in queries:\n",
        "            #print(q)\n",
        "            for ap in queries[q]['ans_patterns']:\n",
        "                rel_anspat[ap.strip()] += bool(re.search(ap.strip(), corpus[doc], flags=re.IGNORECASE))\n",
        "                rel_counts[q-1] += bool(re.search(ap.strip(), corpus[doc], flags=re.IGNORECASE))\n",
        "\n",
        "    return rel_counts, rel_anspat\n",
        "     \n",
        "\n",
        "# Input:\n",
        "# [\n",
        "#    {\n",
        "#       \"query\": \"q1\",\n",
        "#       ID's are such that we can get doc text easily by indexing\n",
        "#       \"rel_docs\": [doc_id1, doc_id2,..... doc_idK], \n",
        "#       \"rel_doc_scores\": [rel_doc1_score, rel_doc2_score,..... rel_docK_score],\n",
        "#    }\n",
        "# ]\n",
        "# \n",
        "# Purpose of this function is to correctly prepare the data so that \n",
        "# it can be preprocessed by MatchZoo lib and then passed on to DRMM model\n",
        "# for getting document score\n",
        "\n",
        "# For ref: rel_docs, scores = get_relevant_docs(test_qs[q], tfidf_reprs, term_idfs, how_many=50)\n",
        "# For ref: df = pd.DataFrame(data={'text_left': list('AABC'), 'text_right': list('abbc')})\n",
        "\n",
        "def prepare_for_reranking(queries, returned_docs):\n",
        "\n",
        "\n",
        "    # Notes for final testing on unseen data\n",
        "    # if we can fit our test corpus in above paradigm, the histogram will be auto computed\n",
        "    # via callbacks. Only thing it needs to calc score is text_left and text_right i.e. \n",
        "    # the X part in (X,y) tuples. Need to see how we can do this.\n",
        "    # X = {text_left, text_right} . we don't need label\n",
        "    \n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    HOW_MANY = 1000 # How many relevant docs to return per query\n",
        "    \n",
        "    corpus = process_trec_xml(trec_corpus_xml, save=True)\n",
        "    term_idfs = compute_term_idfs(corpus, save=True)\n",
        "    tfidf_reprs = compute_tfidf_doc_repr(corpus, term_idfs, save=True)\n",
        "    test_qs = get_test_questions(test_questions, ans_patterns, save=True)\n",
        "\n",
        "    precisions = []\n",
        "\n",
        "    rerank_data = {\n",
        "        'text_left': [], # for queries,\n",
        "        'text_right': [] # for docs \n",
        "    }\n",
        "\n",
        "    for q in test_qs.values():\n",
        "\n",
        "        rel_docs, scores = get_relevant_docs(q, tfidf_reprs, term_idfs, how_many=HOW_MANY)\n",
        "        \n",
        "        # We need 1000 docs for next step, but for report we only need to calc.\n",
        "        # precision@50. So we pass only top-50\n",
        "        precisions.append(\n",
        "            precision_at_r(\n",
        "                rel_docs[:50], \n",
        "                q, \n",
        "                corpus\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # prepping for MatchZoo\n",
        "\n",
        "        rerank_data['text_left'].extend([q['question']] * HOW_MANY)\n",
        "        for doc in rel_docs:\n",
        "            rerank_data['text_right'].append(corpus[doc])\n",
        "    \n",
        "    # We will pass this to our neural / advanced reranking method\n",
        "    # For every query, we store the top 1000 docs\n",
        "    df = pd.DataFrame(data = rerank_data)\n",
        "    unseen_packed_raw = mz.pack(df, task='ranking')\n",
        "    \n",
        "    #########################################################################\n",
        "    # REQUIRED FOR REPORT: \n",
        "    # (g) Sort the similarity scores and output the top 50 most relevant \n",
        "    # documents for a query along with their scores.\n",
        "    #########################################################################\n",
        "    \n",
        "    # print(relevant_per_query(corpus, test_qs))\n",
        "    #print(len(test_qs))\n",
        "    print(\"Precision is: \", sum(precisions) / len(test_qs))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from saved pickle\n",
            "Loading from saved pickle\n",
            "Loading from saved pickle\n",
            "Loading from saved pickle\n",
            "Precision is:  0.07999999999999993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I24lEUXzntHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e10b2d10-391a-4902-d363-3375a8fa8db3"
      },
      "source": [
        "#len(rerank_data['text_left']), len(rerank_data['text_right'])\n",
        "len(unseen_packed_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOzQuA2fY0Cb",
        "colab_type": "text"
      },
      "source": [
        "# Loading Training data (WikiQA / SQuAD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4sn_fFfzX3L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "14036c20-5569-439b-e6a4-5aed95b573e0"
      },
      "source": [
        "# contents of init.ipynb and drmm.ipynb\n",
        "\n",
        "########################################################################\n",
        "# improve over the performance of the baseline IR model by first ranking and \n",
        "# returning the top 1000 documents for a query with the baseline retriever.\n",
        "# Then, you should develop your own method to re-rank these 1000 documents to\n",
        "# return the top 50 documents, which should improve over the top documents \n",
        "# returned by baseline model\n",
        "########################################################################\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matchzoo as mz\n",
        "print('matchzoo version', mz.__version__)\n",
        "\n",
        "ranking_task = mz.tasks.Ranking(losses=mz.losses.RankCrossEntropyLoss(num_neg=10))\n",
        "ranking_task.metrics = [\n",
        "    mz.metrics.NormalizedDiscountedCumulativeGain(k=3),\n",
        "    mz.metrics.NormalizedDiscountedCumulativeGain(k=5),\n",
        "    mz.metrics.MeanAveragePrecision()\n",
        "]\n",
        "\n",
        "print(\"`ranking_task` initialized with metrics\", ranking_task.metrics)\n",
        "print('data loading ...')\n",
        "\n",
        "# Packing code src: \n",
        "# https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/data_pack/pack.py\n",
        "# filtered=True removes the questions without correct answers.\n",
        "# Load data src: https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/datasets/wiki_qa/load_data.py\n",
        "\n",
        "train_pack_raw = mz.datasets.wiki_qa.load_data('train', task=ranking_task)\n",
        "dev_pack_raw   = mz.datasets.wiki_qa.load_data('dev', task=ranking_task, filtered=True)\n",
        "test_pack_raw  = mz.datasets.wiki_qa.load_data('test', task=ranking_task, filtered=True)\n",
        "\n",
        "# In matchzoo text is presented as a left/right comparison task\n",
        "# i.e. left could be `query` and right could be `doc`\n",
        "# and comparison could be either ranking or classification\n",
        "# In WikiQA\n",
        "# text_left = Question\n",
        "# text_right = Sentence\n",
        "\n",
        "# How to make datapacks from python structures: \n",
        "# https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/data_pack/data_pack.py\n",
        "\n",
        "# Notes for final testing on unseen data\n",
        "# if we can fit our test corpus in above paradigm, the histogram will be auto-computed\n",
        "# via callbacks. Only thing it needs to calc score is text_left and text_right i.e. \n",
        "# the X part in (X,y) tuples. Need to see how we can do this.\n",
        "\n",
        "print('data loaded as `train_pack_raw` `dev_pack_raw` `test_pack_raw`')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "matchzoo version 1.1.1\n",
            "`ranking_task` initialized with metrics [normalized_discounted_cumulative_gain@3(0.0), normalized_discounted_cumulative_gain@5(0.0), mean_average_precision(0.0)]\n",
            "data loading ...\n",
            "Downloading data from https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\n",
            "7094272/7094233 [==============================] - 0s 0us/step\n",
            "data loaded as `train_pack_raw` `dev_pack_raw` `test_pack_raw`\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfvYDgrtY_q6",
        "colab_type": "text"
      },
      "source": [
        "# Basic preprocessing with default preprocessor \n",
        "\n",
        "- Tokenization, Stop word removal, Lowercasing, Punctuation Removal etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS6RFyizzhVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "036e4773-7219-441e-89be-82905121f809"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# preprocessor specifies things like stop word removal, length truncation,\n",
        "# frequency based filtering etc.\n",
        "# From output it seems like it does Tokenize => Lowercase => PuncRemoval\n",
        "# src: https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/engine/base_model.py\n",
        "# basic pre-processor: https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/preprocessors/basic_preprocessor.py\n",
        "# Freq Filter: https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/preprocessors/units/frequency_filter.py\n",
        "# By default the truncation on both left and right is 'None' i.e. they are not \n",
        "# truncated to any fixed length. I don't think they're are even needed for DRMM\n",
        "\n",
        "preprocessor = mz.models.DRMM.get_default_preprocessor()\n",
        "\n",
        "# Data pack source : https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/data_pack/data_pack.py\n",
        "\n",
        "train_pack_processed = preprocessor.fit_transform(train_pack_raw)\n",
        "dev_pack_processed = preprocessor.transform(dev_pack_raw)\n",
        "test_pack_processed = preprocessor.transform(test_pack_raw)\n",
        "unseen_pack_processed = preprocessor.transform(unseen_packed_raw)\n",
        "\n",
        "# print some info on vocab size etc\n",
        "print(preprocessor.context)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 2118/2118 [00:00<00:00, 7612.45it/s]\n",
            "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 18841/18841 [00:04<00:00, 3909.58it/s]\n",
            "Processing text_right with append: 100%|██████████| 18841/18841 [00:00<00:00, 877420.55it/s]\n",
            "Building FrequencyFilter from a datapack.: 100%|██████████| 18841/18841 [00:00<00:00, 124879.52it/s]\n",
            "Processing text_right with transform: 100%|██████████| 18841/18841 [00:00<00:00, 135983.22it/s]\n",
            "Processing text_left with extend: 100%|██████████| 2118/2118 [00:00<00:00, 392625.12it/s]\n",
            "Processing text_right with extend: 100%|██████████| 18841/18841 [00:00<00:00, 647606.92it/s]\n",
            "Building Vocabulary from a datapack.: 100%|██████████| 418412/418412 [00:00<00:00, 2236213.90it/s]\n",
            "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 2118/2118 [00:00<00:00, 7702.79it/s]\n",
            "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 18841/18841 [00:04<00:00, 4041.49it/s]\n",
            "Processing text_right with transform: 100%|██████████| 18841/18841 [00:00<00:00, 112649.85it/s]\n",
            "Processing text_left with transform: 100%|██████████| 2118/2118 [00:00<00:00, 163399.41it/s]\n",
            "Processing text_right with transform: 100%|██████████| 18841/18841 [00:00<00:00, 101665.74it/s]\n",
            "Processing length_left with len: 100%|██████████| 2118/2118 [00:00<00:00, 311320.69it/s]\n",
            "Processing length_right with len: 100%|██████████| 18841/18841 [00:00<00:00, 749463.04it/s]\n",
            "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 122/122 [00:00<00:00, 6960.17it/s]\n",
            "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 1115/1115 [00:00<00:00, 4041.42it/s]\n",
            "Processing text_right with transform: 100%|██████████| 1115/1115 [00:00<00:00, 101338.03it/s]\n",
            "Processing text_left with transform: 100%|██████████| 122/122 [00:00<00:00, 61216.07it/s]\n",
            "Processing text_right with transform: 100%|██████████| 1115/1115 [00:00<00:00, 78542.38it/s]\n",
            "Processing length_left with len: 100%|██████████| 122/122 [00:00<00:00, 107455.92it/s]\n",
            "Processing length_right with len: 100%|██████████| 1115/1115 [00:00<00:00, 285300.69it/s]\n",
            "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 237/237 [00:00<00:00, 8034.35it/s]\n",
            "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 2300/2300 [00:00<00:00, 4045.85it/s]\n",
            "Processing text_right with transform: 100%|██████████| 2300/2300 [00:00<00:00, 121384.34it/s]\n",
            "Processing text_left with transform: 100%|██████████| 237/237 [00:00<00:00, 76114.09it/s]\n",
            "Processing text_right with transform: 100%|██████████| 2300/2300 [00:00<00:00, 16853.07it/s]\n",
            "Processing length_left with len: 100%|██████████| 237/237 [00:00<00:00, 111841.81it/s]\n",
            "Processing length_right with len: 100%|██████████| 2300/2300 [00:00<00:00, 307578.73it/s]\n",
            "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 100/100 [00:00<00:00, 6045.84it/s]\n",
            "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 8661/8661 [00:10<00:00, 799.34it/s] \n",
            "Processing text_right with transform: 100%|██████████| 8661/8661 [00:00<00:00, 16816.02it/s]\n",
            "Processing text_left with transform: 100%|██████████| 100/100 [00:00<00:00, 27464.01it/s]\n",
            "Processing text_right with transform: 100%|██████████| 8661/8661 [00:00<00:00, 15247.60it/s]\n",
            "Processing length_left with len: 100%|██████████| 100/100 [00:00<00:00, 62073.46it/s]\n",
            "Processing length_right with len: 100%|██████████| 8661/8661 [00:00<00:00, 783210.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'filter_unit': <matchzoo.preprocessors.units.frequency_filter.FrequencyFilter object at 0x7fe20a5a8ac8>, 'vocab_unit': <matchzoo.preprocessors.units.vocabulary.Vocabulary object at 0x7fe210799f60>, 'vocab_size': 30059, 'embedding_input_dim': 30059}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGm3XYr4Zbdo",
        "colab_type": "text"
      },
      "source": [
        "# Load GLoVE Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-7XBI8VzpX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1c3b8ad1-edae-4803-a847-13e3c16e4a14"
      },
      "source": [
        "# GloVE: https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/datasets/embeddings/load_glove_embedding.py\n",
        "glove_embedding = mz.datasets.embeddings.load_glove_embedding(dimension=300)\n",
        "\n",
        "# Vocab unit src: https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/preprocessors/units/vocabulary.py\n",
        "term_index = preprocessor.context['vocab_unit'].state['term_index']\n",
        "embedding_matrix = glove_embedding.build_matrix(term_index)\n",
        "l2_norm = np.sqrt((embedding_matrix * embedding_matrix).sum(axis=1))\n",
        "embedding_matrix = embedding_matrix / l2_norm[:, np.newaxis]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://nlp.stanford.edu/data/glove.6B.zip\n",
            "862183424/862182613 [==============================] - 390s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFSFdqPhZgpB",
        "colab_type": "text"
      },
      "source": [
        "# Create DataLoaders from pre-processed data\n",
        "\n",
        "- This step also includes a callback which does Histogram computation for each training example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCeX0djtz1SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function of callbacks - Callback is used to transform / compute relevant \n",
        "# statistics on a databatch. e.g. here we compute matching histogram\n",
        "\n",
        "# Base callback source: https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/engine/base_callback.py\n",
        "# Hist callback source: https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/dataloader/callbacks/histogram.py\n",
        "\n",
        "histgram_callback = mz.dataloader.callbacks.Histogram(\n",
        "    embedding_matrix, bin_size=30, hist_mode='LCH'\n",
        ")\n",
        "\n",
        "# src Dataset : https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/dataloader/dataset.py\n",
        "# num_dup: Number of duplications per instance\n",
        "# num_neg: Number of negative samples per instance\n",
        "\n",
        "trainset = mz.dataloader.Dataset(\n",
        "    data_pack=train_pack_processed,\n",
        "    mode='pair',\n",
        "    num_dup=5,\n",
        "    num_neg=10,\n",
        "    callbacks=[histgram_callback]\n",
        ")\n",
        "testset = mz.dataloader.Dataset(\n",
        "    data_pack=test_pack_processed,\n",
        "    callbacks=[histgram_callback]\n",
        ")\n",
        "\n",
        "unseen = mz.dataloader.Dataset(\n",
        "    data_pack=unseen_pack_processed,\n",
        "    batch_size=1000, # So we can get all top-1k ranked docs per query in 10 batch\n",
        "    callbacks=[histgram_callback],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# padding callback src: https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/engine/base_model.py#L226\n",
        "padding_callback = mz.models.DRMM.get_default_padding_callback()\n",
        "\n",
        "# DataLoader src: https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/dataloader/dataloader.py\n",
        "trainloader = mz.dataloader.DataLoader(\n",
        "    device=device,\n",
        "    dataset=trainset,\n",
        "    stage='train',\n",
        "    #resample=True,\n",
        "    callback=padding_callback\n",
        ")\n",
        "testloader = mz.dataloader.DataLoader(\n",
        "    dataset=testset,\n",
        "    device=device,\n",
        "    stage='dev',\n",
        "    callback=padding_callback\n",
        ")\n",
        "\n",
        "# create loader for unseen TREC XML data\n",
        "unseenloader = mz.dataloader.DataLoader(\n",
        "    dataset=unseen,\n",
        "    device=device,\n",
        "    #batch_size=HOW_MANY,\n",
        "    stage='test', # 'test' bcz we don't have Y.\n",
        "    callback=padding_callback\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SRS2XgCZxBh",
        "colab_type": "text"
      },
      "source": [
        "# Initialize DRMM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4x6gB7G1xFi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "cb1c1a29-ae5c-4efc-ee23-1bfaaacff530"
      },
      "source": [
        "# DRMM src - https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/models/drmm.py\n",
        "\n",
        "model = mz.models.DRMM().to(device)\n",
        "\n",
        "# DRMM fwd pass: https://github.com/NTMC-Community/MatchZoo-py/blob/49548ad4dd7da4c890ac786a09d9df9172c3af47/matchzoo/models/drmm.py#L70\n",
        "\n",
        "model.params['task'] = ranking_task\n",
        "model.params['mask_value'] = 0\n",
        "model.params['embedding'] = embedding_matrix\n",
        "model.params['hist_bin_size'] = 30\n",
        "model.params['mlp_num_layers'] = 1\n",
        "model.params['mlp_num_units'] = 10\n",
        "model.params['mlp_num_fan_out'] = 1\n",
        "model.params['mlp_activation_func'] = 'tanh'\n",
        "model.params['embedding_freeze'] = True\n",
        "model.build()\n",
        "\n",
        "print(model.params)\n",
        "print('Trainable params: ', sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_class                   <class 'matchzoo.models.drmm.DRMM'>\n",
            "task                          Ranking Task\n",
            "out_activation_func           None\n",
            "with_embedding                True\n",
            "embedding                     [[-0.04967236  0.01572789  0.07462103 ... -0.07714741 -0.05622825\n",
            "   0.03582055]\n",
            " [-0.08508103  0.05267619 -0.07051839 ...  0.03369272 -0.02393593\n",
            "  -0.08515697]\n",
            " [ 0.022119    0.08201521  0.01799603 ... -0.14142403  0.09055382\n",
            "   0.02254226]\n",
            " ...\n",
            " [ 0.01527015  0.05386136 -0.00782055 ...  0.08603887  0.07638169\n",
            "  -0.07216352]\n",
            " [ 0.09764317 -0.09725617  0.08593716 ... -0.00455429 -0.08631574\n",
            "   0.08131319]\n",
            " [-0.07896867  0.00017807 -0.07912631 ... -0.05847381 -0.00127889\n",
            "  -0.07484334]]\n",
            "embedding_input_dim           30059\n",
            "embedding_output_dim          300\n",
            "padding_idx                   0\n",
            "embedding_freeze              True\n",
            "with_multi_layer_perceptron   True\n",
            "mlp_num_units                 10\n",
            "mlp_num_layers                1\n",
            "mlp_num_fan_out               1\n",
            "mlp_activation_func           tanh\n",
            "mask_value                    0\n",
            "hist_bin_size                 30\n",
            "Trainable params:  623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTtmAd7xPBj9",
        "colab_type": "text"
      },
      "source": [
        "# Checking for bugs in data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdyh5zyOPBLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47a8f385-265d-4295-8d39-413dc8e25739"
      },
      "source": [
        "batch = None\n",
        "for X, _ in unseenloader:\n",
        "  batch = X\n",
        "  break\n",
        "\n",
        "l, r = batch['text_left'], batch['text_right']\n",
        "\n",
        "vocab_unit = preprocessor.context['vocab_unit']\n",
        "term_idx_pad = vocab_unit.transform(['<PAD>'])[0]\n",
        "\n",
        "\n",
        "len(r[[1,3,4,5]])\n",
        "# for q,d in zip(l,r):\n",
        "#   print(\n",
        "#      ' '.join(\n",
        "#          vocab_unit.state['index_term'][i] \\\n",
        "#          for i in q.tolist() \\\n",
        "#          if i != term_idx_pad)\n",
        "#    )\n",
        "#   print()\n",
        "#   print(\n",
        "#      ' '.join(\n",
        "#          vocab_unit.state['index_term'][i] \\\n",
        "#          for i in d.tolist() \\\n",
        "#          if i != term_idx_pad)\n",
        "#    )\n",
        "#   print()\n",
        "#   break\n",
        "\n",
        "# for q in r:\n",
        "#   print( ' '.join(\n",
        "#          vocab_unit.state['index_term'][i] \\\n",
        "#          for i in q.tolist() \\\n",
        "#          if i != term_idx_pad))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3lsZ2fDDyoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "d2246851-37be-4710-cf91-89ae8e6c6945"
      },
      "source": [
        "selected = r[[1,3,4,5]]\n",
        "\n",
        "for s in selected: \n",
        "    print( ' '.join(\n",
        "         vocab_unit.state['index_term'][i] \\\n",
        "         for i in s.tolist() \\\n",
        "         if i != term_idx_pad) )\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "why did lady thatcher come out so strongly in support of john major is as easy as but clarke\n",
            "\n",
            "the recovery in the world iron ore market that began last year picked in 1994 and should accelerate in 1995 according to the conference on trade and says next year should see the reversal of three of declining iron ore iron ore exports rose by nearly 8 per cent to tonnes in to a marked reduction in exporters stocks while global iron rose by 25 per cent to tonnes the stimulus from the economy more than offset a sharp drop in mining and former soviet union the report imported more than tonnes of steel last year and thus iron ore consumption in all the main iron ore imports also soared by 30 per cent confirming as the worlds most dynamic market for current and future iron the largest iron ore producer china mined tonnes last year of nearly 15 per cent over 1992 however domestic production 70 per cent of the needs points out iron rose 166 per cent to tonnes in 1993 or more than a the world says that world iron ore trade remained strong in the first half propelled by even faster growth of chinese demand smaller declines in japan and accelerating economic recovery in most it expects a significant upsurge in global steel and iron in 1995 which augurs well for the recovery of iron ore have come under pressure from leading consuming countries on the and from strong competition among suppliers on the other profitability of the industry and especially says falling prices have not the industry in replacement and additional of the current situation and outlook for iron ore 1994 available from des nations\n",
            "\n",
            "a biography that portrays franklin delano roosevelt as a actor and an examination of how technology has affected human values were among six books named tuesday as winners of the 1990 los angeles times book prizes at a reception for publishers in new york times book editor jack miles announced that geoffrey c ward author of a the emergence of franklin roosevelt had won the biography award and o b jr author of disappearing through the culture and technology in the twentieth century was being honored posthumously as the recipient of the current interest prize each prize carries a 1000 cash award and a copy of the winning book in leather irish novelist won the fiction award for slides a collection of short stories exploring themes of and loss set in ireland italy and london where the writer has lived for the past two decades slides for the times elaine kendall said the collection the authors sixth represents the essential completely irish absolutely universal the wit and anguish in equilibrium has also written 10 novels and several winner of the history prize was richard fletcher author of the quest for el cid a study of the 11th century who became first national hero his subject fletcher who teaches at the university of york in england depicts him as an soldier of fortune who used whatever means were necessary to achieve his goals in the category of science and technology jane s smith was honored for the sun polio and the vaccine a social history of the epidemic that american parents in the 1950s and the scientific discovery that ended their fear smith author of a biography of de the interior lives in ill she was one of 2 million who participated in trials for the vaccine john received the poetry prize for the color of bones a collection of autobiographical poems that describe his suffering at the hands of an abusive father the poems are set in the mountain range in northeastern minnesota teaches at university in st paul winning authors will be honored at a reception at the times building on 2 when the winner of the robert award will be announced that award named for the times late book critic recognizes a writer who has lived in the west or made it the focus of his or her work a is the second volume of wards psychologically biography of roosevelt beginning with marriage in 1905 the author tracks him through his fledgling legal career to his election as governor of new york and argues that the future presidents acting skills enabled him to cope with the effects of paralysis a former editor of audience and american heritage magazines ward is of the civil war the documentary series to be aired on pbs next week he lives in new york roosevelt biographer arthur schlesinger jr has written that wards effort the springs of political genius the books title is drawn from justice oliver holmes famous after meeting roosevelt in 1933 a but a who died of cancer on aug 5 at age 61 was a professor of english at georgetown university who wrote many books of cultural criticism in his last book he traced this major technological developments and at how they have fundamentally altered basic concepts of nature history language art and human evolution author james has described work as lively provocative and often profound if i were to characterize the general feel of his writing i would call it scholarship on fire the times has awarded book prizes since 1980 winners are selected by six committees of three judges each none of the judges are times employees\n",
            "\n",
            "western nations should have threatened to bomb serbian and supply lines if serb forces did not stop and lady thatcher the former british prime a conference in washington this weekend george graham reports\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgmufkpTZ0Ck",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J693dMkd4oo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adadelta(model.parameters())\n",
        "\n",
        "trainer = mz.trainers.Trainer(\n",
        "    device='cpu',\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    trainloader=trainloader,\n",
        "    validloader=testloader,\n",
        "    validate_interval=None,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Hk4E-d4sEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446,
          "referenced_widgets": [
            "c1465f3ff6454636ba77c8a9b1cc176b",
            "a6a2ad394b9c4df482da80050a8b97c0",
            "715ce1dc328144829440a40cb881d6f2",
            "c43781b93c0c47dbaf62a0c32d84d426",
            "4ed7fefc4944438a9b8ceb86022b64a1",
            "85cfefe7547f44ffaaa0392b6257b4c6",
            "991b5ced94f84c7baafeca29d7a20b2d",
            "8280c8b0d1fe41af9c6c82d214f3c7e2",
            "10dd0d21af8e49c8a4b99026125e514d",
            "7528c29877af4083a2f149d8d145167a",
            "0fa785eaeab94d52bec388f0990debb7",
            "a7677e4d900b485aa22a14cd7f496364",
            "704bef1b45cb43c4945488ab0265fee7",
            "8d62b46c6d6c4ac4bd3c62575d692726",
            "73b3cfc9e58b46e698f21aa94c8c693b",
            "fb4aedc0b30b401eae23a9d7668e10b8"
          ]
        },
        "outputId": "fb887630-1fa9-4268-dd8f-94f6e429932d"
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1465f3ff6454636ba77c8a9b1cc176b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=160), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[Iter-160 Loss-2.374]:\n",
            "  Validation: normalized_discounted_cumulative_gain@3(0.0): 0.3794 - normalized_discounted_cumulative_gain@5(0.0): 0.4633 - mean_average_precision(0.0): 0.4298\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10dd0d21af8e49c8a4b99026125e514d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=160), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-041e2033e90a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matchzoo/trainers/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_early_stopping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop_early\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matchzoo/trainers/trainer.py\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         with tqdm(enumerate(self._trainloader), total=num_batch,\n\u001b[1;32m    250\u001b[0m                   disable=not self._verbose) as pbar:\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;31m# Caculate all losses and sum them up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matchzoo/dataloader/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;34m\"\"\"Iteration.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_callbacks_on_batch_unpacked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matchzoo/dataloader/dataset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matchzoo/dataloader/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_callbacks_on_batch_data_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data_pack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data_pack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_callbacks_on_batch_unpacked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matchzoo/dataloader/dataset.py\u001b[0m in \u001b[0;36m_handle_callbacks_on_batch_unpacked\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_callbacks_on_batch_unpacked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_unpacked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matchzoo/dataloader/callbacks/histogram.py\u001b[0m in \u001b[0;36mon_batch_unpacked\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_unpacked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;34m\"\"\"Insert `match_histogram` to `x`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'match_histogram'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_match_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_hist_unit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matchzoo/dataloader/callbacks/histogram.py\u001b[0m in \u001b[0;36m_build_match_histogram\u001b[0;34m(x, match_hist_unit)\u001b[0m\n\u001b[1;32m     62\u001b[0m                              x['length_right'].tolist())\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mmatch_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_hist_unit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matchzoo/preprocessors/units/matching_histogram.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndenumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mbin_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hist_bin_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mmatching_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbin_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NH'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mmatching_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatching_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI2-naDBZ5K3",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGxxaeJ7aOr6",
        "colab_type": "text"
      },
      "source": [
        "# Get Predictions on Unseen (TREC XML) data\n",
        "\n",
        "# Print Question and Top relevant doc for each query, for inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt5tdOZOqLRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "e90c3f67-a3e8-4b21-ae18-a9d6fdb10737"
      },
      "source": [
        "vocab_unit = preprocessor.context['vocab_unit']\n",
        "term_idx_pad = vocab_unit.transform(['<PAD>'])[0]\n",
        "\n",
        "# scores - K doc scores returned by model for a single query \n",
        "def get_top_K_indices(scores,K=50):\n",
        "  \n",
        "  scores_kv = { k:v[0] for k,v in enumerate(scores) }\n",
        "  sorted_scores_kv = sorted(scores_kv.items(), \n",
        "                            key = lambda x: x[1], \n",
        "                            reverse=True)\n",
        "  sorted_indices = [idx for idx, _ in sorted_scores_kv]\n",
        "\n",
        "  # return top K indices\n",
        "  return sorted_indices[:K]\n",
        "\n",
        "# Calculate precision over the returned result set and query\n",
        "# precision = #(rel and ret) / #(ret)\n",
        "def precision_at_r_model(returned_docs_idx, questions):\n",
        "    \n",
        "    precisions = []\n",
        "    i = 1\n",
        "    \n",
        "    for q, docs, (X, _) in zip(questions, returned_docs_idx, unseenloader):\n",
        "      \n",
        "      assert len(docs) == 50, \"R is not 50\"\n",
        "      print(\"Q %d: %s\" % (i, questions[q]['question']))\n",
        "      i = i + 1\n",
        "      \n",
        "      returned_docs = X['text_right'][docs]\n",
        "      relevant_count = 0\n",
        "      \n",
        "      for d_i,d in enumerate(returned_docs):\n",
        "        \n",
        "        d = ' '.join(vocab_unit.state['index_term'][t] \\\n",
        "         for t in d.tolist() \\\n",
        "         if t != 0)\n",
        "        \n",
        "        if d_i == 0: print(d) # print only first doc\n",
        "\n",
        "        rel = []\n",
        "        \n",
        "        for ap in questions[q]['ans_patterns']:\n",
        "            rel.append(bool(re.search(ap.strip(), d, flags=re.IGNORECASE)))\n",
        "            # count as relevant if at least one answer pattern \n",
        "            # matched somewhere in doc\n",
        "        relevant_count += int(any(rel))\n",
        "      print(relevant_count)\n",
        "      precisions.append(relevant_count / 50)\n",
        "\n",
        "    return sum(precisions) / 10\n",
        "\n",
        "per_query_scores = []\n",
        "best_docs = []\n",
        "queries = []\n",
        "limit = 2\n",
        "i = 0\n",
        "b = 1\n",
        "\n",
        "with torch.no_grad():\n",
        "  for X,_ in unseenloader:\n",
        "    \n",
        "    print(\"Batch %d\" % b)\n",
        "    b = b + 1\n",
        "    \n",
        "    # save scores for top-1k docs\n",
        "    pqs = model(X).tolist()\n",
        "    \n",
        "    # get indices of top-scoring 50 docs\n",
        "    best_docs.append(\n",
        "        get_top_K_indices(scores=pqs,K=50)\n",
        "    )\n",
        "    \n",
        "    i = i + 1\n",
        "    if i >= limit:\n",
        "      break # try for 1 first\n",
        "\n",
        "print(\n",
        "    precision_at_r_model(best_docs, test_qs)\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch 1\n",
            "Batch 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-5136947b4ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m print(\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mprecision_at_r_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfQCzuaXWPZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c31a57c2-1777-4f45-fe08-e07bb41d5bdc"
      },
      "source": [
        "#unseenloader.__iter__()\n",
        "test_qs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {'ans_patterns': ['Young'],\n",
              "  'question': 'who is the author of the book the iron lady a biography of margaret thatcher',\n",
              "  'raw_question': 'Who is the author of the book, \"The Iron Lady: A Biography of Margaret Thatcher\"?'},\n",
              " 2: {'ans_patterns': ['\\\\$469,000'],\n",
              "  'question': 'what was the monetary value of the nobel peace prize in 1989',\n",
              "  'raw_question': 'What was the monetary value of the Nobel Peace Prize in 1989?'},\n",
              " 3: {'ans_patterns': ['405',\n",
              "   'automobiles?',\n",
              "   'diesel\\\\s+motors?',\n",
              "   '309s?',\n",
              "   '106s?',\n",
              "   '504s?',\n",
              "   '505s?',\n",
              "   '205s?',\n",
              "   '306s?',\n",
              "   'vehicles?',\n",
              "   'cars?',\n",
              "   'Peugeots',\n",
              "   'plastic\\\\s+components'],\n",
              "  'question': 'what does the peugeot company manufacture',\n",
              "  'raw_question': 'What does the Peugeot company manufacture?'},\n",
              " 4: {'ans_patterns': ['Pounds\\\\s+12\\\\s*(?:m|(?:million))'],\n",
              "  'question': 'how much did mercury spend on advertising in 1993',\n",
              "  'raw_question': 'How much did Mercury spend on advertising in 1993?'},\n",
              " 5: {'ans_patterns': ['Horne'],\n",
              "  'question': 'what is the name of the managing director of apricot computer',\n",
              "  'raw_question': 'What is the name of the managing director of Apricot Computer?'},\n",
              " 6: {'ans_patterns': ['To\\\\s+record\\\\s+his\\\\s+revelations',\n",
              "   'finish\\\\w*\\\\s+writing.*?revelations'],\n",
              "  'question': 'why did david koresh ask the fbi for a word processor',\n",
              "  'raw_question': 'Why did David Koresh ask the FBI for a word processor?'},\n",
              " 7: {'ans_patterns': ['1\\\\.4\\\\s*(?:(?:bn)|(?:billion))',\n",
              "   '1\\\\.6\\\\s*(?:(?:bn)|(?:billion))'],\n",
              "  'question': 'what debts did qintex group leave',\n",
              "  'raw_question': 'What debts did Qintex group leave?'},\n",
              " 8: {'ans_patterns': [\"Tourette\\\\s*'\\\\s*s\"],\n",
              "  'question': 'what is the name of the rare neurological disease with symptoms such as involuntary movements tics swearing and incoherent vocalizations grunts shouts etc',\n",
              "  'raw_question': 'What is the name of the rare neurological disease with symptoms such as: involuntary movements (tics), swearing, and incoherent vocalizations (grunts, shouts, etc.)?'},\n",
              " 9: {'ans_patterns': ['150 miles?'],\n",
              "  'question': 'how far is yaroslavl from moscow',\n",
              "  'raw_question': 'How far is Yaroslavl from Moscow?'},\n",
              " 10: {'ans_patterns': ['Pfister'],\n",
              "  'question': 'name the designer of the shoe that spawned millions of plastic imitations known as jellies',\n",
              "  'raw_question': 'Name the designer of the shoe that spawned millions of plastic imitations, known as \"jellies\".'},\n",
              " 11: {'ans_patterns': ['Folsom'],\n",
              "  'question': 'who was president clevelands wife',\n",
              "  'raw_question': \"Who was President Cleveland's wife?\"},\n",
              " 12: {'ans_patterns': ['Pounds\\\\s*4\\\\s*(?:m|(?:million))'],\n",
              "  'question': 'how much did manchester united spend on players in 1993',\n",
              "  'raw_question': 'How much did Manchester United spend on players in 1993?'},\n",
              " 13: {'ans_patterns': ['\\\\$\\\\s*1'],\n",
              "  'question': 'how much could you rent a volkswagen bug for in 1966',\n",
              "  'raw_question': 'How much could you rent a Volkswagen bug for in 1966?'},\n",
              " 14: {'ans_patterns': ['China'],\n",
              "  'question': 'what country is the biggest producer of tungsten',\n",
              "  'raw_question': 'What country is the biggest producer of tungsten?'},\n",
              " 15: {'ans_patterns': ['1980s', '1987'],\n",
              "  'question': 'when was londons docklands light railway constructed',\n",
              "  'raw_question': \"When was London's Docklands Light Railway constructed?\"},\n",
              " 16: {'ans_patterns': ['(?:Krebs.*?Fischer)|(?:Fischer.*?Krebs)'],\n",
              "  'question': 'what two us biochemists won the nobel prize in medicine in 1992',\n",
              "  'raw_question': 'What two US biochemists won the Nobel Prize in medicine in 1992?'},\n",
              " 17: {'ans_patterns': ['(?:(?:nine)|9).*?months?'],\n",
              "  'question': 'how long did the charles manson murder trial last',\n",
              "  'raw_question': 'How long did the Charles Manson murder trial last?'},\n",
              " 18: {'ans_patterns': ['Lee\\\\s+Teng\\\\s*-?\\\\s*Hui',\n",
              "   'Mr\\\\.?\\\\s+Li',\n",
              "   'Mr\\\\.?\\\\s+Lee',\n",
              "   'Li\\\\s+Teng\\\\s*-?\\\\s*Hui',\n",
              "   'President\\\\s+Li',\n",
              "   'President\\\\s+Lee'],\n",
              "  'question': 'who was the first taiwanese president',\n",
              "  'raw_question': 'Who was the first Taiwanese President?'},\n",
              " 19: {'ans_patterns': ['Koresh'],\n",
              "  'question': 'who was the leader of the branch davidian cult confronted by the fbi in waco texas in 1993',\n",
              "  'raw_question': 'Who was the leader of the Branch Davidian Cult confronted by the FBI in Waco, Texas in 1993?'},\n",
              " 20: {'ans_patterns': ['Norwich'],\n",
              "  'question': 'where is inoco based',\n",
              "  'raw_question': 'Where is Inoco based?'},\n",
              " 21: {'ans_patterns': ['Shepard'],\n",
              "  'question': 'who was the first american in space',\n",
              "  'raw_question': 'Who was the first American in space?'},\n",
              " 22: {'ans_patterns': ['130\\\\s+million\\\\s+years\\\\s+ago'],\n",
              "  'question': 'when did the jurassic period end',\n",
              "  'raw_question': 'When did the Jurassic Period end?'},\n",
              " 23: {'ans_patterns': ['1950'],\n",
              "  'question': 'when did spain and korea start ambassadorial relations',\n",
              "  'raw_question': 'When did Spain and Korea start ambassadorial relations?'},\n",
              " 24: {'ans_patterns': ['today',\n",
              "   'two\\\\s+decades\\\\s+ago',\n",
              "   '2[23]\\\\s+years\\\\s+ago',\n",
              "   '17\\\\s+years\\\\s+ago',\n",
              "   '1972',\n",
              "   'more\\\\s+than\\\\s+20\\\\s+years\\\\s+have\\\\s+elapsed',\n",
              "   'over\\\\s+20\\\\s+years\\\\s+ago',\n",
              "   'April\\\\s+1993',\n",
              "   'February\\\\s+1976',\n",
              "   'early\\\\s+1970s'],\n",
              "  'question': 'when did nixon visit china',\n",
              "  'raw_question': 'When did Nixon visit China?'},\n",
              " 25: {'ans_patterns': ['Ryan'],\n",
              "  'question': 'who was the lead actress in the movie sleepless in seattle',\n",
              "  'raw_question': 'Who was the lead actress in the movie \"Sleepless in Seattle\"?'},\n",
              " 26: {'ans_patterns': ['La\\\\s+Nina'],\n",
              "  'question': 'what is the name of the female counterpart to el nino which results in cooling temperatures and very dry weather',\n",
              "  'raw_question': 'What is the name of the \"female\" counterpart to El Nino, which results in cooling temperatures and very dry weather?'},\n",
              " 27: {'ans_patterns': ['Surabaya', 'East\\\\s+Java'],\n",
              "  'question': 'where did the 6th annual meeting of indonesiamalaysia forest experts take place',\n",
              "  'raw_question': 'Where did the 6th annual meeting of Indonesia-Malaysia forest experts take place?'},\n",
              " 28: {'ans_patterns': ['Robinson'],\n",
              "  'question': 'who may be best known for breaking the color line in baseball',\n",
              "  'raw_question': 'Who may be best known for breaking the color line in baseball?'},\n",
              " 29: {'ans_patterns': ['Sirius'],\n",
              "  'question': 'what is the brightest star visible from earth',\n",
              "  'raw_question': 'What is the brightest star visible from Earth?'},\n",
              " 30: {'ans_patterns': ['10\\\\s*-?\\\\s*point\\\\s+environmental\\\\s+agenda',\n",
              "   'environmental\\\\s+agenda\\\\s+for\\\\s+corporations',\n",
              "   'principles\\\\s+of\\\\s+the\\\\s+Coalition\\\\s+of\\\\s+Environmentally\\\\s+Responsible\\\\s+Economies',\n",
              "   'code\\\\s+of\\\\s+conduct'],\n",
              "  'question': 'what are the valdez principles',\n",
              "  'raw_question': 'What are the Valdez Principles?'},\n",
              " 31: {'ans_patterns': ['Ohio'],\n",
              "  'question': 'where was ulysses s grant born',\n",
              "  'raw_question': 'Where was Ulysses S. Grant born?'},\n",
              " 32: {'ans_patterns': ['Sinatra'],\n",
              "  'question': 'who received the will rogers award in 1989',\n",
              "  'raw_question': 'Who received the Will Rogers Award in 1989?'},\n",
              " 33: {'ans_patterns': ['Berlin'],\n",
              "  'question': 'what is the largest city in germany',\n",
              "  'raw_question': 'What is the largest city in Germany?'},\n",
              " 34: {'ans_patterns': ['Hollywood\\\\s+Memorial\\\\s+Park',\n",
              "   'Hollywood\\\\s+Cemetery'],\n",
              "  'question': 'where is the actress marion davies buried',\n",
              "  'raw_question': 'Where is the actress, Marion Davies, buried?'},\n",
              " 35: {'ans_patterns': ['Kilimanjaro', 'Uhuru\\\\s+Peak'],\n",
              "  'question': 'what is the name of the highest mountain in africa',\n",
              "  'raw_question': 'What is the name of the highest mountain in Africa?'},\n",
              " 36: {'ans_patterns': ['Tuesday'],\n",
              "  'question': 'in 1990 what day of the week did christmas fall on',\n",
              "  'raw_question': 'In 1990, what day of the week did Christmas fall on?'},\n",
              " 37: {'ans_patterns': ['Hall'],\n",
              "  'question': 'what was the name of the us helicopter pilot shot down over north korea',\n",
              "  'raw_question': 'What was the name of the US helicopter pilot shot down over North Korea?'},\n",
              " 38: {'ans_patterns': ['Westmoreland\\\\s+County', 'Virginia'],\n",
              "  'question': 'where was george washington born',\n",
              "  'raw_question': 'Where was George Washington born?'},\n",
              " 39: {'ans_patterns': ['Powell'],\n",
              "  'question': 'who was chosen to be the first black chairman of the military joint chiefs of staff',\n",
              "  'raw_question': 'Who was chosen to be the first black chairman of the military Joint Chiefs of Staff?'},\n",
              " 40: {'ans_patterns': ['Kyi'],\n",
              "  'question': 'who won the nobel peace prize in 1991',\n",
              "  'raw_question': 'Who won the Nobel Peace Prize in 1991?'},\n",
              " 41: {'ans_patterns': ['0?\\\\.\\\\s*08(?:%|(?:pct)|(?:per))?',\n",
              "   '0?\\\\.\\\\s*10(?:%|(?:pct)|(?:per))?',\n",
              "   'twice\\\\s+the\\\\s+legal\\\\s+limit.*?\\\\.20?'],\n",
              "  'question': 'what is the legal blood alcohol limit for the state of california',\n",
              "  'raw_question': 'What is the legal blood alcohol limit for the state of California?'},\n",
              " 42: {'ans_patterns': ['3\\\\s*\\\\.\\\\s*5\\\\s*(?:-|(?:to))\\\\s*5\\\\s*\\\\.\\\\s*5(?:%|(?:pct)|(?:per))?',\n",
              "   '4\\\\s*(?:-|(?:to))\\\\s*6(?:%|(?:pct)|(?:per))?',\n",
              "   'between\\\\s+3\\\\s*\\\\.\\\\s*5.*?5\\\\s*\\\\.\\\\s*5(?:%|(?:pct)|(?:per))?',\n",
              "   '3\\\\s+1/2\\\\s*(?:-|(?:to))\\\\s*5\\\\s+1/2(?:%|(?:pct)|(?:per))?'],\n",
              "  'question': 'what was the target rate for m3 growth in 1992',\n",
              "  'raw_question': 'What was the target rate for M3 growth in 1992?'},\n",
              " 43: {'ans_patterns': ['Whitten'],\n",
              "  'question': 'what costume designer decided that michael jackson should only wear one glove',\n",
              "  'raw_question': 'What costume designer decided that Michael Jackson should only wear one glove?'},\n",
              " 44: {'ans_patterns': ['McKusick'],\n",
              "  'question': 'who is the director of the international group called the human genome organization hugo that is trying to coordinate genemapping research worldwide',\n",
              "  'raw_question': 'Who is the director of the international group called the Human Genome Organization (HUGO) that is trying to coordinate gene-mapping research worldwide?'},\n",
              " 45: {'ans_patterns': ['10\\\\s*Feb(?:ruary)?', 'Feb\\\\s*10', 'Feb.*?94'],\n",
              "  'question': 'when did lucelly garcia a former ambassador of columbia to honduras die',\n",
              "  'raw_question': 'When did Lucelly Garcia, a former ambassador of Columbia to Honduras, die?'},\n",
              " 46: {'ans_patterns': ['Jesus\\\\s+Gil\\\\s+y\\\\s+Gil'],\n",
              "  'question': 'who is the mayor of marbella',\n",
              "  'raw_question': 'Who is the mayor of Marbella?'},\n",
              " 47: {'ans_patterns': ['Mitsubishi\\\\s+Heavy\\\\s+Industries'],\n",
              "  'question': 'what company is the largest japanese ship builder',\n",
              "  'raw_question': 'What company is the largest Japanese ship builder?'},\n",
              " 48: {'ans_patterns': ['Yongbyon',\n",
              "   'Yongbyun',\n",
              "   '90\\\\s*km\\\\s+north\\\\s+of\\\\s+Pyongyang'],\n",
              "  'question': 'where is the massive north korean nuclear complex located',\n",
              "  'raw_question': 'Where is the massive North Korean nuclear complex located?'},\n",
              " 49: {'ans_patterns': ['Henderson'],\n",
              "  'question': 'who fired maria ybarra from her position in san diego council',\n",
              "  'raw_question': 'Who fired Maria Ybarra from her position in San Diego council?'},\n",
              " 50: {'ans_patterns': ['1956'],\n",
              "  'question': 'when was dubais first concrete house built',\n",
              "  'raw_question': \"When was Dubai's first concrete house built?\"},\n",
              " 51: {'ans_patterns': ['Kennedy'],\n",
              "  'question': 'who is the president of stanford university',\n",
              "  'raw_question': 'Who is the president of Stanford University?'},\n",
              " 52: {'ans_patterns': ['Morgan'],\n",
              "  'question': 'who invented the road traffic cone',\n",
              "  'raw_question': 'Who invented the road traffic cone?'},\n",
              " 53: {'ans_patterns': ['Starzl'],\n",
              "  'question': 'who was the first doctor to successfully transplant a liver',\n",
              "  'raw_question': 'Who was the first doctor to successfully transplant a liver?'},\n",
              " 54: {'ans_patterns': ['Apr(?:il)?\\\\s*22', '22\\\\s*Apr(?:il)?'],\n",
              "  'question': 'when did nixon die',\n",
              "  'raw_question': 'When did Nixon die?'},\n",
              " 55: {'ans_patterns': ['Washington', 'Seattle\\\\s+suburb', 'around\\\\s+Seattle'],\n",
              "  'question': 'where is microsofts corporate headquarters located',\n",
              "  'raw_question': \"Where is Microsoft's corporate headquarters located?\"},\n",
              " 56: {'ans_patterns': ['562'],\n",
              "  'question': 'how many calories are there in a big mac',\n",
              "  'raw_question': 'How many calories are there in a Big Mac?'},\n",
              " 57: {'ans_patterns': ['S?EER'],\n",
              "  'question': 'what is the acronym for the rating system for air conditioner efficiency',\n",
              "  'raw_question': 'What is the acronym for the rating system for air conditioner efficiency?'},\n",
              " 58: {'ans_patterns': ['In\\\\s+The\\\\s+Name\\\\s+Of\\\\s+The\\\\s+Father',\n",
              "   'Music\\\\s+Box',\n",
              "   'Grand\\\\s+Canyon',\n",
              "   '(?:Sky)?larks\\\\s+on\\\\s+the\\\\s+String',\n",
              "   'Larks',\n",
              "   'Mistertao',\n",
              "   'The\\\\s+Wedding\\\\s+Banquet'],\n",
              "  'question': 'name a film that has won the golden bear in the berlin film festival',\n",
              "  'raw_question': 'Name a film that has won the Golden Bear in the Berlin Film Festival?'},\n",
              " 59: {'ans_patterns': ['Calderon', 'Figueres'],\n",
              "  'question': 'who was president of costa rica in 1994',\n",
              "  'raw_question': 'Who was President of Costa Rica in 1994?'},\n",
              " 60: {'ans_patterns': ['\\\\$?6\\\\s*,?\\\\s*400', 'Pounds\\\\s+5\\\\s*,?\\\\s*0[03]0'],\n",
              "  'question': 'what is the fare cost for the round trip between new york and london on concorde',\n",
              "  'raw_question': 'What is the fare cost for the round trip between New York and London on Concorde?'},\n",
              " 61: {'ans_patterns': ['Havana\\\\s+Club'],\n",
              "  'question': 'what brand of white rum is still made in cuba',\n",
              "  'raw_question': 'What brand of white rum is still made in Cuba?'},\n",
              " 62: {'ans_patterns': ['Multiple\\\\s+Sclerosis', 'MS'],\n",
              "  'question': 'what is the name of the chronic neurological autoimmune disease which attacks the protein sheath that surrounds nerve cells causing a gradual loss of movement in the body',\n",
              "  'raw_question': 'What is the name of the chronic neurological autoimmune disease which attacks the protein sheath that surrounds nerve cells causing a gradual loss of movement in the body?'},\n",
              " 63: {'ans_patterns': ['Komsomolets'],\n",
              "  'question': 'what nuclearpowered russian submarine sank in the norwegian sea on april 7 1989',\n",
              "  'raw_question': 'What nuclear-powered Russian submarine sank in the Norwegian Sea on April 7, 1989?'},\n",
              " 64: {'ans_patterns': ['Frank\\\\s+Oz'],\n",
              "  'question': 'who is the voice of miss piggy',\n",
              "  'raw_question': 'Who is the voice of Miss Piggy?'},\n",
              " 65: {'ans_patterns': ['Japan(?:(?:ese)|s)?', 'German(?:y|s)?'],\n",
              "  'question': 'name a country that is developing a magnetic levitation railway system',\n",
              "  'raw_question': 'Name a country that is developing a magnetic levitation railway system?'},\n",
              " 66: {'ans_patterns': ['McAuliffe'],\n",
              "  'question': 'name the first private citizen to fly in space',\n",
              "  'raw_question': 'Name the first private citizen to fly in space.'},\n",
              " 67: {'ans_patterns': ['Mississippi'],\n",
              "  'question': 'what is the longest river in the united states',\n",
              "  'raw_question': 'What is the longest river in the United States?'},\n",
              " 68: {'ans_patterns': ['boy\\\\s+child', 'Christ\\\\s+child'],\n",
              "  'question': 'what does el nino mean in spanish',\n",
              "  'raw_question': 'What does El Nino mean in spanish?'},\n",
              " 69: {'ans_patterns': ['fishermen'],\n",
              "  'question': 'who came up with the name el nino ',\n",
              "  'raw_question': 'Who came up with the name, El Nino? '},\n",
              " 70: {'ans_patterns': ['26[1234]', 'some\\\\s+220'],\n",
              "  'question': 'how many lives were lost in the china airlines crash in nagoya japan',\n",
              "  'raw_question': \"How many lives were lost in the China Airlines' crash in Nagoya, Japan?\"},\n",
              " 71: {'ans_patterns': ['1941'],\n",
              "  'question': 'in what year did joe dimaggio compile his 56game hitting streak',\n",
              "  'raw_question': 'In what year did Joe DiMaggio compile his 56-game hitting streak?'},\n",
              " 72: {'ans_patterns': ['1960'],\n",
              "  'question': 'when did the original howdy doody show go off the air',\n",
              "  'raw_question': 'When did the original Howdy Doody show go off the air?'},\n",
              " 73: {'ans_patterns': ['Agra', 'India'],\n",
              "  'question': 'where is the taj mahal',\n",
              "  'raw_question': 'Where is the Taj Mahal?'},\n",
              " 74: {'ans_patterns': ['Kirk'],\n",
              "  'question': 'who leads the star ship enterprise in star trek',\n",
              "  'raw_question': 'Who leads the star ship Enterprise in Star Trek?'},\n",
              " 75: {'ans_patterns': ['Kaposi'],\n",
              "  'question': 'what cancer is commonly associated with aids',\n",
              "  'raw_question': 'What cancer is commonly associated with AIDS?'},\n",
              " 76: {'ans_patterns': ['198[567]', \"mid\\\\s*-?\\\\s*1980\\\\s*'?\\\\s*s\"],\n",
              "  'question': 'in which year was new zealand excluded from the anzus alliance',\n",
              "  'raw_question': 'In which year was New Zealand excluded from the ANZUS alliance?'},\n",
              " 77: {'ans_patterns': ['Brando'],\n",
              "  'question': 'who played the part of the godfather in the movie the godfather ',\n",
              "  'raw_question': 'Who played the part of the Godfather in the movie, \"The Godfather\"? '},\n",
              " 78: {'ans_patterns': [\"Washington\\\\s*'?\\\\s*s?\", 'District\\\\s+of\\\\s+Columbia'],\n",
              "  'question': 'which large us city had the highest murder rate for 1988',\n",
              "  'raw_question': 'Which large U.S. city had the highest murder rate for 1988?'},\n",
              " 79: {'ans_patterns': ['cello\\\\s+concertos?'],\n",
              "  'question': 'what did shostakovich write for rostropovich',\n",
              "  'raw_question': 'What did Shostakovich write for Rostropovich?'},\n",
              " 80: {'ans_patterns': ['taxol'],\n",
              "  'question': 'what is the name of the promising anticancer compound derived from the pacific yew tree',\n",
              "  'raw_question': 'What is the name of the promising anticancer compound derived from the pacific yew tree?'},\n",
              " 81: {'ans_patterns': ['30\\\\s*,?\\\\s*000'],\n",
              "  'question': 'how many inhabitants live in the town of ushuaia',\n",
              "  'raw_question': 'How many inhabitants live in the town of Ushuaia?'},\n",
              " 82: {'ans_patterns': ['2\\\\s*,?\\\\s*130'],\n",
              "  'question': 'how many consecutive baseball games did lou gehrig play',\n",
              "  'raw_question': 'How many consecutive baseball games did Lou Gehrig play?'},\n",
              " 83: {'ans_patterns': ['Sunshine', 'Landmark\\\\s+Tower'],\n",
              "  'question': 'what is the tallest building in japan',\n",
              "  'raw_question': 'What is the tallest building in Japan?'},\n",
              " 84: {'ans_patterns': ['Japan'],\n",
              "  'question': 'which country is australias largest export market',\n",
              "  'raw_question': \"Which country is Australia's largest export market?\"},\n",
              " 85: {'ans_patterns': ['Duke'],\n",
              "  'question': 'which former ku klux klan member won an elected office in the us',\n",
              "  'raw_question': 'Which former Ku Klux Klan member won an elected office in the U.S.?'},\n",
              " 86: {'ans_patterns': ['Tomba'],\n",
              "  'question': 'who won two gold medals in skiing in the olympic games in calgary',\n",
              "  'raw_question': 'Who won two gold medals in skiing in the Olympic Games in Calgary?'},\n",
              " 87: {'ans_patterns': ['Helmut\\\\s+Schmidt'],\n",
              "  'question': 'who followed willy brandt as chancellor of the federal republic of germany',\n",
              "  'raw_question': 'Who followed Willy Brandt as chancellor of the Federal Republic of Germany?'},\n",
              " 88: {'ans_patterns': ['nutmeg'],\n",
              "  'question': 'what is grenadas main commodity export',\n",
              "  'raw_question': \"What is Grenada's main commodity export?\"},\n",
              " 89: {'ans_patterns': [\"mid\\\\s*-?\\\\s*30\\\\s*'?\\\\s*s\"],\n",
              "  'question': 'at what age did rossini stop writing opera',\n",
              "  'raw_question': 'At what age did Rossini stop writing opera?'},\n",
              " 90: {'ans_patterns': ['Hubbard'],\n",
              "  'question': 'who is the founder of scientology',\n",
              "  'raw_question': 'Who is the founder of Scientology?'},\n",
              " 91: {'ans_patterns': ['Shanghai'],\n",
              "  'question': 'which city in china has the largest number of foreign financial companies',\n",
              "  'raw_question': 'Which city in China has the largest number of foreign financial companies?'},\n",
              " 92: {'ans_patterns': ['Morris'],\n",
              "  'question': 'who released the internet worm in the late 1980s',\n",
              "  'raw_question': 'Who released the Internet worm in the late 1980s?'},\n",
              " 93: {'ans_patterns': ['Magellan'],\n",
              "  'question': 'who first circumnavigated the globe',\n",
              "  'raw_question': 'Who first circumnavigated the globe?'},\n",
              " 94: {'ans_patterns': ['Hoagie\\\\s+Carmichael'],\n",
              "  'question': 'who wrote the song stardust',\n",
              "  'raw_question': 'Who wrote the song, \"Stardust\"?'},\n",
              " 95: {'ans_patterns': ['Ghana', 'Morocco'],\n",
              "  'question': 'what country is the worlds leading supplier of cannabis',\n",
              "  'raw_question': 'What country is the worlds leading supplier of cannabis?'},\n",
              " 96: {'ans_patterns': ['6:33\\\\s*a\\\\.?m\\\\.?'],\n",
              "  'question': 'what time of day did emperor hirohito die',\n",
              "  'raw_question': 'What time of day did Emperor Hirohito die?'},\n",
              " 97: {'ans_patterns': ['19m\\\\s*-?\\\\s*acres?'],\n",
              "  'question': 'how large is the arctic refuge to preserve unique wildlife and wilderness value on alaskas north coast',\n",
              "  'raw_question': \"How large is the Arctic refuge to preserve unique wildlife and wilderness value on Alaska's north coast?\"},\n",
              " 98: {'ans_patterns': ['Mt\\\\.?\\\\s+Fuji', '12\\\\s*,?\\\\s*388\\\\s*ft\\\\.?'],\n",
              "  'question': 'where is the highest point in japan',\n",
              "  'raw_question': 'Where is the highest point in Japan?'},\n",
              " 99: {'ans_patterns': ['genome'],\n",
              "  'question': 'what is the term for the sum of all genetic material in a given organism',\n",
              "  'raw_question': 'What is the term for the sum of all genetic material in a given organism?'},\n",
              " 100: {'ans_patterns': ['Hurricane\\\\s+Hugo', 'Hurricane\\\\s+Andrew'],\n",
              "  'question': 'what is considered the costliest disaster the insurance industry has ever faced',\n",
              "  'raw_question': 'What is considered the costliest disaster the insurance industry has ever faced?'}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc3bLnlp6Mqc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "658f5c12-dc14-4a3e-a0a3-cd5752a3cdfb"
      },
      "source": [
        "precision_at_r_model(\n",
        "    returned_docs_idx, \n",
        "    test_qs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ans_patterns': ['Young'],\n",
              " 'question': 'who is the author of the book the iron lady a biography of margaret thatcher',\n",
              " 'raw_question': 'Who is the author of the book, \"The Iron Lady: A Biography of Margaret Thatcher\"?'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxCoo_T2B9ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}